import re
import os

# --- Configuration ---
DATA_DIR = "data"
AIRLINES_CORPUS_FILE = os.path.join(DATA_DIR, "airlines_corpus.txt")
ADDITIONAL_WORDS_FILE = os.path.join(DATA_DIR, "airlines.txt") 
OUTPUT_FILE = os.path.join(DATA_DIR, "airlines_corpus.txt")  

MAX_WORD_LENGTH = 25
MIN_AVG_PHRASE_LENGTH = 2.5

WHITELIST = {
    "hub", "roll", "pitch", "go first", "leh", "neo","vor", "ils", "gps", "pax", "ramp", "tow", "apu", "mel", "vfr", "ifr",
    "atc", "jet", "suv", "yaw", "fin", "rib", "spar", "lift", "drag",
    "fbo", "notam", "tcas", "gpws", "aog", "mro", "dgca", "squawk",
    "jetway", "chocks", "deadhead", "pan pan", "slot", "flare", "stall", "baggage","galley","hub","hypoxia",
    "logbook","mayday","notam","pilot","pushback","rvsm","squawk","vacate","waypoint","windshear alert","yield"


}

STOP_WORDS = {
    # Single letters
    'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm',
    'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z',
    # Conjunctions
    'for', 'and', 'nor', 'but', 'or', 'yet', 'so', 'after', 'although', 'as', 
    'because', 'before', 'how', 'if', 'once', 'since', 'than', 'that', 'though', 
    'till', 'until', 'when', 'where', 'whether', 'while',
    # Prepositions
    'about', 'above', 'across', 'against', 'along', 'among', 'around', 
    'at', 'behind', 'below', 'beneath', 'beside', 'between', 'beyond', 
    'by', 'down', 'during', 'from', 'in', 'inside', 'into', 'like', 
    'near', 'of', 'off', 'on', 'onto', 'out', 'outside', 'over', 'past', 
    'through', 'to', 'toward', 'under', 'underneath', 'unto', 
    'up', 'upon', 'with', 'within', 'without',
    # Other common words
    'all', 'am', 'an', 'any', 'are', 'be', 'been', 'being', 'both', 'can', 
    'did', 'do', 'does', 'doing', 'each', 'few', 'further', 'had', 'has', 
    'have', 'having', 'he', 'her', 'here', 'hers', 'herself', 'him', 'himself', 
    'his', 'is', 'its', 'itself', 'just', 'me', 'more', 'most', 'my', 
    'myself', 'no', 'not', 'now', 'our', 'same', 'she', 'should', 'so', 
    'some', 'such', 'the', 'their', 'theirs', 'them', 'then', 'there', 'these', 
    'they', 'this', 'those', 'too', 'very', 'was', 'we', 'were', 'what', 
    'which', 'who', 'whom', 'why', 'will', 'you', 'your', 'yours', 
    'yourself',
    # Web Junk
    'retrieved', 'archived', 'https', 'http', 'www', 'com', 'org', 'net', 'pdf',
    # Additional irrelevant words
    'camel', 'crabs', 'dolphin', 'donkeys', 'goose', 'horse', 'lion', 'lobster', 'monkey',
    'mosquito', 'oysters', 'panda', 'penguin', 'pig', 'rabbit', 'salmon', 'snake', 'tiger', 'whale',
    'zebra', 'alcohol', 'apple', 'baking', 'banana', 'beer', 'berries', 'cheese', 'chocolate', 'cinnamon',
    'coffee', 'flour', 'ginger', 'honey', 'mango', 'meat', 'milk', 'nuts', 'onion', 'pizza', 'rice', 'salad',
    'salt', 'sandwich', 'soup', 'spices', 'sugar', 'syrup', 'tea', 'tomato', 'vinegar', 'wine', 'yogurt', 'agreement',
    'banking', 'billion', 'budget', 'business', 'commerce', 'contract', 'corruption', 'currency', 'customer', 'economy',
    'finance', 'investment', 'market', 'million', 'money', 'payment', 'profit', 'sale', 'shares', 'tax', 'bamboo', 'beach',
    'cloud', 'desert', 'earthquake', 'flower', 'forest', 'garden', 'grass', 'ice', 'island', 'jungle', 'lake', 'mountain', 'ocean',
    'river', 'rose', 'stone', 'tree', 'volcano', 'water', 'weather', 'ambulance', 'architecture', 'bicycle', 'book', 'brick', 'bridge',
    'button', 'camera', 'carpet', 'chair', 'clothing', 'cosmetics', 'democracy', 'dictionary', 'doctor', 'door', 'dream', 'dress', 'electricity',
    'encyclopedia', 'family', 'father', 'football', 'furniture', 'ghost', 'government', 'gunpowder', 'hammer', 'hospital', 'hotel', 'house',
    'internet', 'kitchen', 'knife', 'letter', 'library', 'magazine', 'medicine', 'mirror', 'mother', 'motorcycle', 'movie', 'museum', 'music',
    'newspaper', 'office', 'paint', 'paper', 'perfume', 'phone', 'photo', 'pirate', 'plumber', 'police', 'president', 'prison', 'radio',
    'restaurant', 'robot', 'school', 'scissors', 'shampoo', 'shoes', 'soap', 'soldier', 'sport', 'student', 'submarine', 'table', 'taxi',
    'teacher', 'telephone', 'television', 'theater', 'thief', 'ticket', 'tourist', 'tractor', 'train', 'truck', 'umbrella', 'university',
    'video', 'war', 'wedding', 'window', 'yacht', 'zoo'
}


def refine_corpus():
    """
    Reads raw corpora, cleans them, combines with additional word lists,
    and overwrites the main corpus file with the refined version.
    """
    print("Starting corpus refinement...")

    raw_corpus = set()
    additional_words = set()

    # Load the primary raw corpus generated by the scraper
    try:
        with open(AIRLINES_CORPUS_FILE, 'r', encoding='utf-8') as f:
            raw_corpus = {line.strip() for line in f if line.strip()}
        print(f"Loaded {len(raw_corpus)} raw entries from '{os.path.basename(AIRLINES_CORPUS_FILE)}'.")
    except FileNotFoundError:
        print(f"Warning: '{os.path.basename(AIRLINES_CORPUS_FILE)}' not found. It will be created from other sources.")
    except Exception as e:
        print(f"Error loading '{os.path.basename(AIRLINES_CORPUS_FILE)}': {e}")

    # Load the additional words from the new data file
    try:
        # Ensure the 'data' directory exists before trying to read from it
        os.makedirs(os.path.dirname(ADDITIONAL_WORDS_FILE), exist_ok=True)
        with open(ADDITIONAL_WORDS_FILE, 'r', encoding='utf-8') as f:
            additional_words = {line.strip().lower() for line in f if line.strip()}
        print(f"Loaded {len(additional_words)} additional words from '{os.path.basename(ADDITIONAL_WORDS_FILE)}'.")
    except FileNotFoundError:
        print(f"Info: Additional words file '{os.path.basename(ADDITIONAL_WORDS_FILE)}' not found. Skipping.")
    except Exception as e:
        print(f"Error loading '{os.path.basename(ADDITIONAL_WORDS_FILE)}': {e}")

    # Combine all sources into one large set to process
    full_raw_corpus = raw_corpus.union(additional_words)

    # Start with our guaranteed VIP words from the whitelist
    refined_corpus = set(WHITELIST)

    for entry in full_raw_corpus:
        # Rule 1: Basic junk filter (must only contain letters and spaces)
        if not re.match(r'^[a-z\s]+$', entry):
            continue

        words = entry.split()

        # Rule 2: "Alphabet Soup" Filter
        if len(words) > 1:
            avg_len = sum(len(word) for word in words) / len(words)
            if avg_len < MIN_AVG_PHRASE_LENGTH:
                continue

        # Rule 3: Gibberish Filter (unnaturally long words)
        if any(len(word) > MAX_WORD_LENGTH for word in words):
            continue

        # Rule 4: Final Decision Logic (Trust the source, filter stop words)
        if len(words) == 1:
            if entry not in STOP_WORDS:
                refined_corpus.add(entry)
        else:
            refined_corpus.add(entry)

    print(f"Refined corpus down to {len(refined_corpus)} high-quality entries.")

    # Sort the final list for consistency
    sorted_refined_corpus = sorted(list(refined_corpus))

    try:
        with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
            for word in sorted_refined_corpus:
                f.write(f"{word}\n")
        print(f"Successfully saved the refined corpus to '{os.path.basename(OUTPUT_FILE)}'.")
    except IOError as e:
        print(f"Error: Could not write to the output file. {e}")


if __name__ == "__main__":
    refine_corpus()

